{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reddit20for20.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbAEQJUAXVFL"
      },
      "source": [
        "# Reddit 20 for 20 Scraping Workshop Code (for Google Colab Only)\n",
        "## Presented By: Graham Schuckman | gschuckm@terpmail.umd.edu\n",
        "Documentation: https://praw.readthedocs.io/en/latest/getting_started/quick_start.html\n",
        "GitHub Repository: https://github.com/grahamschuckman/20for20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP4_c6DbyliQ"
      },
      "source": [
        "# Colab does not come with the praw library by default, so we need to install it\n",
        "!pip install praw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9-A5AZ2x1Gw"
      },
      "source": [
        "\"\"\"\n",
        "This script accepts system arguments for specific subreddits.\n",
        "The last system argument should be the number of posts requested.\n",
        "Ex: python reddit_api.py funny memes 100\n",
        "\"\"\"\n",
        "\n",
        "# We need to import the following libraries to be able to properly run our code\n",
        "import praw\n",
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime\n",
        "import sys\n",
        "\n",
        "# Create a client for the Reddit API using the following credentials\n",
        "reddit = praw.Reddit(client_id='',  # Put the string under \"personal use script\" here\n",
        "                     client_secret='',  # Put the secret here\n",
        "                     user_agent='',  # Put the name of your app here\n",
        "                     username='',  # Put your reddit username here\n",
        "                     password='')  # Put your reddit password here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqO1oh2NzS8A"
      },
      "source": [
        "# Check to make sure we successfully created the client\n",
        "print(reddit.user.me())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4XXd-e6sNly"
      },
      "source": [
        "### Note: Reddit is recommending we use the Async PRAW to avoid sleep commands, but since we are not running a sophisticated bot that is constantly posting and expecting results back, we can continue with the regular PRAW. Plus, we can avoid entering await commands: https://github.com/praw-dev/asyncpraw"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDfzXdXozP_s"
      },
      "source": [
        "# Since we are testing this in Colab, we need to simulate entering in command line arguments\n",
        "# We can test this by using the example command given in the docstring, but we will only ask for 5 messages\n",
        "sys.argv = input('Please enter the command to run your program: ').split(' ')\n",
        "\n",
        "# Our arguments will be read in as a list, just like it would be from the command line\n",
        "print(sys.argv, type(sys.argv))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4PPmdBEDny1"
      },
      "source": [
        "# When running from the command line, the 'python' argument does not get included, so we need to remove it\n",
        "sys.argv = sys.argv[1:len(sys.argv)]\n",
        "print(sys.argv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCs8FcxeD_Aa"
      },
      "source": [
        "# Count the system arguments (subreddit names) and subtract 1 since the script name will always be included\n",
        "arguments = len(sys.argv) - 1\n",
        "print(arguments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xK-MNAq1d0d"
      },
      "source": [
        "# If the user forgets to enter in any system arguments, we should exit the program and inform them of the error\n",
        "if arguments == 0:\n",
        "    sys.exit(\"Please provide at least one valid subreddit name. Ex: python reddit_api.py funny memes 100\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbXxUi_OFSCo"
      },
      "source": [
        "# Assign the last system argument to be the number of posts retrieved from each subreddit\n",
        "last_argument = len(sys.argv) - 1\n",
        "\n",
        "# Exit the script if no number of posts system argument was provided or it was not provided as an integer\n",
        "try:\n",
        "    num_messages = int(sys.argv[last_argument])\n",
        "    print(num_messages)\n",
        "    \n",
        "except:\n",
        "    sys.exit(\"Enter the last argument as the number of posts to retrieve. Ex: python reddit_api.py funny memes 100\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzhI7zlkHGUO"
      },
      "source": [
        "# Now we are ready to begin scraping data from the Reddit API\n",
        "# We can test our scraper by manually providing an argument before we convert the code into a loop\n",
        "# We will use the first subreddit argument which is at position 1 (remember, lists are 0-based in Python)\n",
        "position = 1\n",
        "\n",
        "# Let us make sure we can pass this argument to our Reddit client and get some results\n",
        "try:\n",
        "    # Choose to examine a given subreddit\n",
        "    subreddit_name = sys.argv[position]\n",
        "    subreddit = reddit.subreddit(subreddit_name)\n",
        "\n",
        "    # Returns a list-like object with the top (newest = subreddit.new) n submissions to the subreddit (limit of 1000)\n",
        "    top_subreddit = subreddit.top(limit=num_messages)\n",
        "\n",
        "# If we pass in an invalid subreddit name like asjhasdasdiugqwdb, we want to handle that error appropriately\n",
        "except:\n",
        "    sys.exit(\"Please confirm that all subreddit names are valid. Ex: python reddit_api.py funny memes 100\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47dPd1wsIZaL"
      },
      "source": [
        "# Let's take a look at how the data is passed to us from Reddit\n",
        "print(top_subreddit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_o9vxSwJFFc"
      },
      "source": [
        "# The data is returned in a Python object called a ListingGenerator, which allows us to loop through it\n",
        "# If we were to loop through normally, we would only get the ID of each item in the ListingGenerator\n",
        "# Using the pprint library, we can actually see the attributes associated with the objects themselves\n",
        "import pprint\n",
        "for submission in subreddit.top(limit=1):\n",
        "    pprint.pprint(vars(submission))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDZUVbR9Muu0"
      },
      "source": [
        "# We can see there are attributes like author, title, created, etc., that we can pull\n",
        "# To store these attributes, we can create a dictionary that we will populate with the values we want\n",
        "# Each key in the dictionary represents an attribute we will scrape, and the values will be lists of the attributes\n",
        "reddit_dict = {\"author\": [],\n",
        "                \"title\": [],\n",
        "                \"score\": [],\n",
        "                \"id\": [], \n",
        "                \"url\": [],\n",
        "                \"comms_num\": [],\n",
        "                \"created\": [],\n",
        "                \"body\": [],\n",
        "                \"subscribers\": [],\n",
        "                \"subreddit\": []}\n",
        "print(reddit_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Qe9q7Z9QNbx"
      },
      "source": [
        "# When you loop through a ListingGenerator, it expires. We need to recreate it so that we can finish scraping.\n",
        "# Choose to examine a given subreddit\n",
        "subreddit_name = sys.argv[position]\n",
        "subreddit = reddit.subreddit(subreddit_name)\n",
        "\n",
        "# Returns a list-like object with the top (newest = subreddit.new) n submissions to the subreddit (limit of 1000)\n",
        "top_subreddit = subreddit.top(limit=num_messages)\n",
        "print(top_subreddit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDpuHT72IVxv"
      },
      "source": [
        "# We can iterate through the top subreddit posts and append to the dictionary\n",
        "for submission in top_subreddit:\n",
        "    # The author must be converted to a string because of the way it is contained in the Redditor class\n",
        "    reddit_dict[\"author\"].append(str(submission.author))\n",
        "    reddit_dict[\"title\"].append(submission.title)\n",
        "    reddit_dict[\"score\"].append(submission.score)\n",
        "    reddit_dict[\"id\"].append(submission.id)\n",
        "    reddit_dict[\"url\"].append(submission.url)\n",
        "    reddit_dict[\"comms_num\"].append(submission.num_comments)\n",
        "    reddit_dict[\"created\"].append(submission.created)\n",
        "    reddit_dict[\"body\"].append(submission.selftext)\n",
        "    reddit_dict[\"subscribers\"].append(submission.subreddit_subscribers)\n",
        "    reddit_dict[\"subreddit\"].append(subreddit_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2Gpj7htRMNq"
      },
      "source": [
        "# Let's take a look at how our dictionary is currently formatted\n",
        "print(reddit_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ipzun0IZONsN"
      },
      "source": [
        "# We can see that the data is there, but it's not very intelligible\n",
        "# To make our data easier to investigate, we can pass the dictionary into a dataframe for easier viewing\n",
        "reddit_data = pd.DataFrame(reddit_dict)\n",
        "reddit_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f-xhafSOvyA"
      },
      "source": [
        "# The formatting of the created column is not very intelligible. We can change it to ISO from UNIX/UTC\n",
        "def get_date(created):\n",
        "    return datetime.utcfromtimestamp(created).isoformat() + '+00:00'\n",
        "\n",
        "# Apply the above function to the \"created\" column, and then rename it to timestamp for easier understanding\n",
        "reddit_data[\"created\"] = reddit_data[\"created\"].apply(get_date)\n",
        "reddit_data.rename(columns = {\"created\" : \"timestamp\"}, inplace = True)\n",
        "\n",
        "# Examine the dataframe and confirm that our data is structured the way we want\n",
        "reddit_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBiPXdqERpHZ"
      },
      "source": [
        "# Convert the dataframe back to a dictionary for rapid exporting to JSON (dataframes are slow)\n",
        "reddit_dict = reddit_data.to_dict('records')\n",
        "\n",
        "# This results in a series of key-value pairs that looks very similar to JSON\n",
        "reddit_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4cSqTGBUp8u"
      },
      "source": [
        "# Now we want to export our JSON data back to our desktops\n",
        "from google.colab import files\n",
        "\n",
        "# Export reddit data to JSON and name the files appropriately\n",
        "with open(subreddit_name + \"_subreddit.json\", 'w+') as f:\n",
        "    json.dump(reddit_dict, f)\n",
        "\n",
        "files.download(subreddit_name + \"_subreddit.json\") \n",
        "print(\"All data has been successfully exported to a JSON file.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC6NYX9NebQa"
      },
      "source": [
        "## Display JSON Content Cleanly\n",
        "http://jsonviewer.stack.hu/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ5RGYKmZ5Tc"
      },
      "source": [
        "## Below is the completed script with looping (3 extra lines)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vanzsem_F0HK"
      },
      "source": [
        "\"\"\"\n",
        "This script accepts system arguments for specific subreddits.\n",
        "The last system argument should be the number of posts requested.\n",
        "Ex: python reddit_api.py funny memes 100\n",
        "\"\"\"\n",
        "\n",
        "# We need to import the following libraries to be able to properly run our code\n",
        "import praw\n",
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime\n",
        "import sys\n",
        "\n",
        "# Create a client for the Reddit API using the following credentials\n",
        "reddit = praw.Reddit(client_id='',  # Put the string under \"personal use script\" here\n",
        "                     client_secret='',  # Put the secret here\n",
        "                     user_agent='',  # Put the name of your app here\n",
        "                     username='',  # Put your reddit username here\n",
        "                     password='')  # Put your reddit password here\n",
        "\n",
        "# Since we are testing this in Colab, we need to simulate entering in command line arguments\n",
        "# We can test this by using the example command given in the docstring, but we will only ask for 5 messages\n",
        "sys.argv = input('Please enter the command to run your program: ').split(' ')\n",
        "\n",
        "# When running from the command line, the 'python' argument does not get included, so we need to remove it\n",
        "sys.argv = sys.argv[1:len(sys.argv)]\n",
        "\n",
        "# Count the system arguments (subreddit names) and subtract 1 since the script name will always be included\n",
        "arguments = len(sys.argv) - 1\n",
        "\n",
        "# If the user forgets to enter in any system arguments, we should exit the program and inform them of the error\n",
        "if arguments == 0:\n",
        "    sys.exit(\"Please provide at least one valid subreddit name. Ex: python reddit_api.py funny memes 100\")\n",
        "\n",
        "# Assign the last system argument to be the number of posts retrieved from each subreddit\n",
        "last_argument = len(sys.argv) - 1\n",
        "\n",
        "# Exit the script if no number of posts system argument was provided or it was not provided as an integer\n",
        "try:\n",
        "    num_messages = int(sys.argv[last_argument])\n",
        "    print(num_messages)\n",
        "    \n",
        "except:\n",
        "    sys.exit(\"Enter the last argument as the number of posts to retrieve. Ex: python reddit_api.py funny memes 100\")\n",
        "\n",
        "# Now we are ready to begin scraping data from the Reddit API\n",
        "# We need to loop through each subreddit argument and get the requested number of posts for each subreddit\n",
        "# We will use the first subreddit argument which is at position 1 (remember, lists are 0-based in Python)\n",
        "position = 1\n",
        "while (arguments > position):\n",
        "    print(sys.argv[position])\n",
        "\n",
        "    try:\n",
        "        # Choose to examine a given subreddit\n",
        "        subreddit_name = sys.argv[position]\n",
        "        subreddit = reddit.subreddit(subreddit_name)\n",
        "\n",
        "        # Returns a list-like object with the top (newest = subreddit.new) n submissions to the subreddit (limit of 1000)\n",
        "        top_subreddit = subreddit.top(limit=num_messages)\n",
        "\n",
        "    # If we pass in an invalid subreddit name like asjhasdasdiugqwdb, we want to handle that error appropriately\n",
        "    except:\n",
        "        sys.exit(\"Please confirm that all subreddit names are valid. Ex: python reddit_api.py funny memes 100\")\n",
        "\n",
        "    # Create a dictionary to store data\n",
        "    reddit_dict = {\"author\": [],\n",
        "                   \"title\": [],\n",
        "                   \"score\": [],\n",
        "                   \"id\": [], \"url\": [],\n",
        "                   \"comms_num\": [],\n",
        "                   \"created\": [],\n",
        "                   \"body\": [],\n",
        "                   \"subscribers\": [],\n",
        "                   \"subreddit\": []}\n",
        "\n",
        "    # Iterate through the top subreddit posts and append to the dictionary\n",
        "    for submission in top_subreddit:\n",
        "        # The author must be converted to a string because of the way it is contained in the Redditor class\n",
        "        reddit_dict[\"author\"].append(str(submission.author))\n",
        "        reddit_dict[\"title\"].append(submission.title)\n",
        "        reddit_dict[\"score\"].append(submission.score)\n",
        "        reddit_dict[\"id\"].append(submission.id)\n",
        "        reddit_dict[\"url\"].append(submission.url)\n",
        "        reddit_dict[\"comms_num\"].append(submission.num_comments)\n",
        "        reddit_dict[\"created\"].append(submission.created)\n",
        "        reddit_dict[\"body\"].append(submission.selftext)\n",
        "        reddit_dict[\"subscribers\"].append(submission.subreddit_subscribers)\n",
        "        reddit_dict[\"subreddit\"].append(subreddit_name)\n",
        "\n",
        "    # Pass the dictionary into a dataframe for easier viewing\n",
        "    reddit_data = pd.DataFrame(reddit_dict)\n",
        "    reddit_data.head()\n",
        "\n",
        "    # The formatting of the created column is not very intelligible. We can change it to ISO from UNIX/UTC\n",
        "    def get_date(created):\n",
        "        return datetime.utcfromtimestamp(created).isoformat() + '+00:00'\n",
        "\n",
        "    # Apply the above function to the \"created\" column, and then rename it to timestamp for easier understanding\n",
        "    reddit_data[\"created\"] = reddit_data[\"created\"].apply(get_date)\n",
        "    reddit_data.rename(columns = {\"created\" : \"timestamp\"}, inplace = True)\n",
        "\n",
        "    # Convert the dataframe back to a dictionary for rapid exporting to JSON\n",
        "    reddit_dict = reddit_data.to_dict('records')\n",
        "\n",
        "    # Now we want to export our JSON data back to our desktops\n",
        "    from google.colab import files\n",
        "\n",
        "    # Export reddit data to JSON and name the files appropriately\n",
        "    with open(subreddit_name + \"_subreddit.json\", 'w+') as f:\n",
        "        json.dump(reddit_dict, f)\n",
        "\n",
        "    files.download(subreddit_name + \"_subreddit.json\") \n",
        "    print(\"All data has been successfully exported to a JSON file.\")\n",
        "\n",
        "    # Loop to the next system argument\n",
        "    position = position + 1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}